{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "\n",
    "# I want to re-randomize the inputs, I am not convinced they are shuffled well\n",
    "total_dataset = np.concatenate((train_dataset, valid_dataset, test_dataset), axis=0)\n",
    "total_labels = np.concatenate((train_labels, valid_labels, test_labels), axis=0)\n",
    "total_dataset, total_labels = randomize(total_dataset, total_labels)\n",
    "train_start = 0\n",
    "valid_start = train_dataset.shape[0]\n",
    "test_start = valid_start + valid_dataset.shape[0]\n",
    "\n",
    "train_dataset = total_dataset[train_start:valid_start, :]\n",
    "train_labels = total_labels[train_start:valid_start] \n",
    "\n",
    "valid_dataset = total_dataset[valid_start:test_start, :]\n",
    "valid_labels = total_labels[valid_start:test_start] \n",
    "\n",
    "test_dataset = total_dataset[test_start:, :]\n",
    "test_labels = total_labels[test_start:] \n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Base implementation for reference\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Epoch  0 ==========\n",
      "Minibatch loss at step 6249: 4.469460\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.2%\n",
      "========== Epoch  1 ==========\n",
      "Minibatch loss at step 6249: 9.242760\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy: 82.0%\n",
      "========== Epoch  2 ==========\n",
      "Minibatch loss at step 6249: 10.132338\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 82.5%\n",
      "Test accuracy: 82.3%\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_width = 1024\n",
    "batch_size = 32\n",
    "num_epochs = 3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random values following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  w1 = tf.Variable(\n",
    "      tf.truncated_normal([image_size * image_size, hidden_layer_width]))\n",
    "  b1 = tf.Variable(tf.zeros([hidden_layer_width]))\n",
    "    \n",
    "  w_final = tf.Variable(\n",
    "      tf.truncated_normal([hidden_layer_width, num_labels]))\n",
    "  b_final = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  layer1_activations = tf.nn.relu(tf.matmul(tf_train_dataset, w1) + b1)\n",
    "  final_activations = tf.matmul(layer1_activations, w_final) + b_final\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=final_activations))\n",
    "  \n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  train_prediction = tf.nn.softmax(final_activations)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "      tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, w1) + b1), w_final) + b_final)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "      tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, w1) + b1), w_final) + b_final)\n",
    "\n",
    "num_steps = train_dataset.shape[0] // batch_size\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      tf.global_variables_initializer().run()\n",
    "      print(\"==========\", \"Epoch \", epoch, \"==========\")\n",
    "  \n",
    "      for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step == num_steps - 1):\n",
    "          print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "          print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "          print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "  \n",
    "      if (epoch == num_epochs - 1) :\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1 (L2 Regularization)\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Epoch  0 ==========\n",
      "Minibatch loss at step 6249: 3.593288\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 85.8%\n",
      "========== Epoch  1 ==========\n",
      "Minibatch loss at step 6249: 3.853190\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 86.0%\n",
      "========== Epoch  2 ==========\n",
      "Minibatch loss at step 6249: 3.934860\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 86.2%\n",
      "Test accuracy: 85.9%\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_width = 1024\n",
    "batch_size = 32\n",
    "num_epochs = 3\n",
    "beta = 0.005 # l2 regularization param\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random values following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  w1 = tf.Variable(\n",
    "      tf.truncated_normal([image_size * image_size, hidden_layer_width]))\n",
    "  b1 = tf.Variable(tf.zeros([hidden_layer_width]))\n",
    "    \n",
    "  w_final = tf.Variable(\n",
    "      tf.truncated_normal([hidden_layer_width, num_labels]))\n",
    "  b_final = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  layer1_activations = tf.nn.relu(tf.matmul(tf_train_dataset, w1) + b1)\n",
    "  final_activations = tf.matmul(layer1_activations, w_final) + b_final\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=final_activations)) + beta * tf.nn.l2_loss(w1)\n",
    "  \n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  train_prediction = tf.nn.softmax(final_activations)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "      tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, w1) + b1), w_final) + b_final)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "      tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, w1) + b1), w_final) + b_final)\n",
    "\n",
    "num_steps = train_dataset.shape[0] // batch_size\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      tf.global_variables_initializer().run()\n",
    "      print(\"==========\", \"Epoch \", epoch, \"==========\")\n",
    "  \n",
    "      for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step == num_steps - 1):\n",
    "          print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "          print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "          print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "  \n",
    "      if (epoch == num_epochs - 1) :\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2 (Overfitting)\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "Answer: Accuracy drops from ~85% --> ~65%\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Epoch  0 ==========\n",
      "Minibatch loss at step 6249: 2.994679\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.9%\n",
      "========== Epoch  1 ==========\n",
      "Minibatch loss at step 6249: 2.994991\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 64.6%\n",
      "========== Epoch  2 ==========\n",
      "Minibatch loss at step 6249: 2.991493\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Test accuracy: 65.7%\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_width = 1024\n",
    "batch_size = 32\n",
    "num_epochs = 3\n",
    "beta = 0.005 # l2 regularization param\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random values following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  w1 = tf.Variable(\n",
    "      tf.truncated_normal([image_size * image_size, hidden_layer_width]))\n",
    "  b1 = tf.Variable(tf.zeros([hidden_layer_width]))\n",
    "    \n",
    "  w_final = tf.Variable(\n",
    "      tf.truncated_normal([hidden_layer_width, num_labels]))\n",
    "  b_final = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  layer1_activations = tf.nn.relu(tf.matmul(tf_train_dataset, w1) + b1)\n",
    "  final_activations = tf.matmul(layer1_activations, w_final) + b_final\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=final_activations)) + beta * tf.nn.l2_loss(w1)\n",
    "  \n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  train_prediction = tf.nn.softmax(final_activations)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "      tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, w1) + b1), w_final) + b_final)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "      tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, w1) + b1), w_final) + b_final)\n",
    "\n",
    "num_steps = train_dataset.shape[0] // batch_size\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      tf.global_variables_initializer().run()\n",
    "      print(\"==========\", \"Epoch \", epoch, \"==========\")\n",
    "  \n",
    "      for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        # offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Only use 10 batches\n",
    "        offset = (step * batch_size) % (5 * batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step == num_steps - 1):\n",
    "          print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "          print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "          print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "  \n",
    "      if (epoch == num_epochs - 1) :\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3 (Dropout)\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "Answer: We go from ~65% accuracy to more around 74%\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfit + Dropout\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Epoch  0 ==========\n",
      "Minibatch loss at step 6249: 5.484126\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 72.5%\n",
      "========== Epoch  1 ==========\n",
      "Minibatch loss at step 6249: 4.219392\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.7%\n",
      "========== Epoch  2 ==========\n",
      "Minibatch loss at step 6249: 4.082470\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.1%\n",
      "Test accuracy: 75.5%\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_width = 1024\n",
    "batch_size = 32\n",
    "num_epochs = 3\n",
    "beta = 0.005 # l2 regularization param\n",
    "learn_rate = 0.1\n",
    "dropout_percent = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random values following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  w1 = tf.Variable(\n",
    "      tf.truncated_normal([image_size * image_size, hidden_layer_width]))\n",
    "  b1 = tf.Variable(tf.zeros([hidden_layer_width]))\n",
    "    \n",
    "  w_final = tf.Variable(\n",
    "      tf.truncated_normal([hidden_layer_width, num_labels]))\n",
    "  b_final = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  layer1_activations = tf.nn.dropout(tf.nn.relu(tf.matmul(tf_train_dataset, w1) + b1), 1.0 - dropout_percent)\n",
    "  final_activations = tf.matmul(layer1_activations, w_final) + b_final\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=final_activations)) + beta * tf.nn.l2_loss(w1)\n",
    "  \n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  train_prediction = tf.nn.softmax(final_activations)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "      tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, w1) + b1), w_final) + b_final)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "      tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, w1) + b1), w_final) + b_final)\n",
    "    \n",
    "num_steps = train_dataset.shape[0] // batch_size\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      tf.global_variables_initializer().run()\n",
    "      print(\"==========\", \"Epoch \", epoch, \"==========\")\n",
    "  \n",
    "      for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        # offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        offset = (step * batch_size) % (10 * batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step == num_steps - 1):\n",
    "          print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "          print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "          print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "  \n",
    "      if (epoch == num_epochs - 1) :\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good pipeline + dropout\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Epoch  0 ==========\n",
      "Minibatch loss at step 6249: 3.345323\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.9%\n",
      "========== Epoch  1 ==========\n",
      "Minibatch loss at step 6249: 1.021154\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 85.5%\n",
      "========== Epoch  2 ==========\n",
      "Minibatch loss at step 6249: 0.373631\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 86.5%\n",
      "Test accuracy: 85.8%\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_width = 1024\n",
    "batch_size = 32\n",
    "num_epochs = 3\n",
    "beta = 0.005 # l2 regularization param\n",
    "learn_rate = 0.1\n",
    "dropout_percent = 0.3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random values following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  w1 = tf.Variable(\n",
    "      tf.truncated_normal([image_size * image_size, hidden_layer_width]))\n",
    "  b1 = tf.Variable(tf.zeros([hidden_layer_width]))\n",
    "    \n",
    "  w_final = tf.Variable(\n",
    "      tf.truncated_normal([hidden_layer_width, num_labels]))\n",
    "  b_final = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  layer1_activations = tf.nn.dropout(tf.nn.relu(tf.matmul(tf_train_dataset, w1) + b1), 1.0 - dropout_percent)\n",
    "  final_activations = tf.matmul(layer1_activations, w_final) + b_final\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=final_activations)) + beta * tf.nn.l2_loss(w1)\n",
    "  \n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  train_prediction = tf.nn.softmax(final_activations)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "      tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, w1) + b1), w_final) + b_final)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "      tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, w1) + b1), w_final) + b_final)\n",
    "    \n",
    "num_steps = train_dataset.shape[0] // batch_size\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "      train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "\n",
    "      print(\"==========\", \"Epoch \", epoch, \"==========\")\n",
    "  \n",
    "      for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step == num_steps - 1):\n",
    "          print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "          print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "          print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "  \n",
    "      if (epoch == num_epochs - 1) :\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4 (Get Gud)\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    "Best Impl so far\n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Epoch  0 ==========\n",
      "\\\\\\\\\\\\\n",
      "Minibatch loss at step 1561: 1.363245\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.7%\n",
      "Training set accuracy: 90.0%\n",
      "////////////\n",
      "========== Epoch  1 ==========\n",
      "\\\\\\\\\\\\\n",
      "Minibatch loss at step 1561: 1.142367\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.2%\n",
      "Training set accuracy: 92.0%\n",
      "////////////\n",
      "========== Epoch  2 ==========\n",
      "\\\\\\\\\\\\\n",
      "Minibatch loss at step 1561: 0.975863\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.5%\n",
      "Training set accuracy: 93.3%\n",
      "////////////\n",
      "========== Epoch  3 ==========\n",
      "\\\\\\\\\\\\\n",
      "Minibatch loss at step 1561: 0.946627\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.2%\n",
      "Training set accuracy: 94.3%\n",
      "////////////\n",
      "========== Epoch  4 ==========\n",
      "\\\\\\\\\\\\\n",
      "Minibatch loss at step 1561: 0.727329\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.3%\n",
      "Training set accuracy: 95.0%\n",
      "////////////\n",
      "========== Epoch  5 ==========\n",
      "\\\\\\\\\\\\\n",
      "Minibatch loss at step 1561: 0.815085\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.4%\n",
      "Training set accuracy: 95.5%\n",
      "////////////\n",
      "========== Epoch  6 ==========\n",
      "\\\\\\\\\\\\\n",
      "Minibatch loss at step 1561: 0.770674\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.4%\n",
      "Training set accuracy: 96.0%\n",
      "////////////\n",
      "========== Epoch  7 ==========\n",
      "\\\\\\\\\\\\\n",
      "Minibatch loss at step 1561: 0.714752\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.8%\n",
      "Training set accuracy: 96.2%\n",
      "////////////\n",
      "========== Epoch  8 ==========\n",
      "\\\\\\\\\\\\\n",
      "Minibatch loss at step 1561: 0.760906\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.8%\n",
      "Training set accuracy: 96.5%\n",
      "////////////\n",
      "========== Epoch  9 ==========\n",
      "\\\\\\\\\\\\\n",
      "Minibatch loss at step 1561: 0.692542\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.8%\n",
      "Training set accuracy: 96.6%\n",
      "////////////\n",
      "Test accuracy: 91.2%\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_width = [2048, 1024]\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "beta = 0.0005 # l2 regularization param\n",
    "start_learn_rate = 0.2 # Seems to be about the max stable number\n",
    "end_learn_rate = 0.01\n",
    "dropout_percent = 0.0\n",
    "\n",
    "# Derived parameters\n",
    "num_steps = train_dataset.shape[0] // batch_size\n",
    "learn_rate_n_steps = num_steps * num_epochs\n",
    "learn_rate_decay = end_learn_rate / start_learn_rate\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_batch_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_batch_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  \n",
    "  tf_train_dataset = tf.constant(train_dataset)\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random values following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  \n",
    "  global_step = tf.Variable(0, trainable=False)  \n",
    "  w1 = tf.Variable(\n",
    "      tf.truncated_normal([image_size * image_size, hidden_layer_width[0]], stddev=np.sqrt(2.0 / hidden_layer_width[0])))\n",
    "  b1 = tf.Variable(tf.zeros([hidden_layer_width[0]]))\n",
    "\n",
    "  w2 = tf.Variable(\n",
    "      tf.truncated_normal([hidden_layer_width[0], hidden_layer_width[1]], stddev=np.sqrt(2.0 / hidden_layer_width[1])))\n",
    "  b2 = tf.Variable(tf.zeros([hidden_layer_width[1]]))\n",
    "    \n",
    "  w_final = tf.Variable(\n",
    "      tf.truncated_normal([hidden_layer_width[1], num_labels], stddev=np.sqrt(2.0 / num_labels)))\n",
    "  b_final = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  layer1_activations = tf.nn.dropout(tf.nn.relu(tf.matmul(tf_batch_dataset, w1) + b1), 1.0 - dropout_percent)\n",
    "  layer2_activations = tf.nn.dropout(tf.nn.relu(tf.matmul(layer1_activations, w2) + b2), 1.0 - dropout_percent)\n",
    "  final_activations = tf.matmul(layer2_activations, w_final) + b_final\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_batch_labels, logits=final_activations))\n",
    "  loss = loss + beta * (tf.nn.l2_loss(w1) + tf.nn.l2_loss(b1) + tf.nn.l2_loss(w2) + tf.nn.l2_loss(b2))\n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  learn_rate = tf.train.exponential_decay(start_learn_rate, global_step, learn_rate_n_steps, learn_rate_decay)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  batch_prediction = tf.nn.softmax(final_activations)\n",
    "  \n",
    "  valid_prediction_l1 = tf.nn.relu(tf.matmul(tf_valid_dataset, w1) + b1)\n",
    "  valid_prediction_l2 = tf.nn.relu(tf.matmul(valid_prediction_l1, w2) + b2)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(valid_prediction_l2, w_final) + b_final)\n",
    "\n",
    "  test_prediction_l1 = tf.nn.relu(tf.matmul(tf_test_dataset, w1) + b1)\n",
    "  test_prediction_l2 = tf.nn.relu(tf.matmul(test_prediction_l1, w2) + b2)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(test_prediction_l2, w_final) + b_final)\n",
    "    \n",
    "  train_prediction_l1 = tf.nn.relu(tf.matmul(tf_train_dataset, w1) + b1)\n",
    "  train_prediction_l2 = tf.nn.relu(tf.matmul(train_prediction_l1, w2) + b2)\n",
    "  train_prediction = tf.nn.softmax(tf.matmul(train_prediction_l2, w_final) + b_final)\n",
    "\n",
    "original_train_labels = train_labels\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "      train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "      print(\"==========\", \"Epoch \", epoch, \"==========\")\n",
    "  \n",
    "      for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_batch_dataset : batch_data, tf_batch_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, batch_prediction], feed_dict=feed_dict)\n",
    "        if (step == num_steps - 1):\n",
    "          print(\"\\\\\\\\\\\\\\\\\\\\\\\\\")\n",
    "          print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "          print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "          print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "          valid_prediction.eval(), valid_labels))\n",
    "          print(\"Training set accuracy: %.1f%%\" % accuracy(train_prediction.eval(), original_train_labels))\n",
    "          print(\"////////////\")\n",
    "  \n",
    "      if (epoch == num_epochs - 1) :\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support.' +\n",
       "              'Please try Chrome, Safari or Firefox â‰¥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option)\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width, fig.canvas.height);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to  previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        event.shiftKey = false;\n",
       "        // Send a \"J\" for go to next cell\n",
       "        event.which = 74;\n",
       "        event.keyCode = 74;\n",
       "        manager.command_mode();\n",
       "        manager.handle_keydown(event);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"638.888905813665\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 91.0%\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_width = [2048, 1024, 512, 512]\n",
    "batch_size = 128\n",
    "num_epochs = 30\n",
    "beta = 0.005  #0.005 # l2 regularization param\n",
    "start_learn_rate = 0.1 # Seems to be about the max stable number\n",
    "end_learn_rate = 0.001\n",
    "dropout_percent = [0.3, 0.2, 0.2, 0.1]\n",
    "\n",
    "# Derived parameters\n",
    "num_steps = train_dataset.shape[0] // batch_size\n",
    "learn_rate_n_steps = num_steps * num_epochs\n",
    "learn_rate_decay = end_learn_rate / start_learn_rate\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_batch_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_batch_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  \n",
    "  tf_train_dataset = tf.constant(train_dataset)\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random values following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  \n",
    "  global_step = tf.Variable(0, trainable=False)  \n",
    "  w1 = tf.Variable(\n",
    "      tf.truncated_normal([image_size * image_size, hidden_layer_width[0]], stddev=np.sqrt(2.0 / hidden_layer_width[0])))\n",
    "  b1 = tf.Variable(tf.zeros([hidden_layer_width[0]]))\n",
    "\n",
    "  w2 = tf.Variable(\n",
    "      tf.truncated_normal([hidden_layer_width[0], hidden_layer_width[1]], stddev=np.sqrt(2.0 / hidden_layer_width[1])))\n",
    "  b2 = tf.Variable(tf.zeros([hidden_layer_width[1]]))\n",
    "\n",
    "  w3 = tf.Variable(\n",
    "      tf.truncated_normal([hidden_layer_width[1], hidden_layer_width[2]], stddev=np.sqrt(2.0 / hidden_layer_width[2])))\n",
    "  b3 = tf.Variable(tf.zeros([hidden_layer_width[2]]))\n",
    "\n",
    "  w4 = tf.Variable(\n",
    "      tf.truncated_normal([hidden_layer_width[2], hidden_layer_width[3]], stddev=np.sqrt(2.0 / hidden_layer_width[3])))\n",
    "  b4 = tf.Variable(tf.zeros([hidden_layer_width[3]]))\n",
    "    \n",
    "  w_final = tf.Variable(\n",
    "      tf.truncated_normal([hidden_layer_width[3], num_labels], stddev=np.sqrt(2.0 / num_labels)))\n",
    "  b_final = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  layer1_activations = tf.nn.dropout(tf.nn.relu(tf.matmul(tf_batch_dataset, w1) + b1), 1.0 - dropout_percent[0])\n",
    "  layer2_activations = tf.nn.dropout(tf.nn.relu(tf.matmul(layer1_activations, w2) + b2), 1.0 - dropout_percent[1])\n",
    "  layer3_activations = tf.nn.dropout(tf.nn.relu(tf.matmul(layer2_activations, w3) + b3), 1.0 - dropout_percent[2])\n",
    "  layer4_activations = tf.nn.dropout(tf.nn.relu(tf.matmul(layer3_activations, w4) + b4), 1.0 - dropout_percent[3])\n",
    "  final_activations = tf.matmul(layer4_activations, w_final) + b_final\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_batch_labels, logits=final_activations))\n",
    "  loss = loss + beta * (tf.nn.l2_loss(w1) + tf.nn.l2_loss(b1) + \n",
    "                        tf.nn.l2_loss(w2) + tf.nn.l2_loss(b2) + \n",
    "                        tf.nn.l2_loss(w3) + tf.nn.l2_loss(b3) + \n",
    "                        tf.nn.l2_loss(w4) + tf.nn.l2_loss(b4))\n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  learn_rate = tf.train.exponential_decay(start_learn_rate, global_step, learn_rate_n_steps, learn_rate_decay)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  batch_prediction = tf.nn.softmax(final_activations)\n",
    "  \n",
    "  valid_prediction_l1 = tf.nn.relu(tf.matmul(tf_valid_dataset, w1) + b1)\n",
    "  valid_prediction_l2 = tf.nn.relu(tf.matmul(valid_prediction_l1, w2) + b2)\n",
    "  valid_prediction_l3 = tf.nn.relu(tf.matmul(valid_prediction_l2, w3) + b3)\n",
    "  valid_prediction_l4 = tf.nn.relu(tf.matmul(valid_prediction_l3, w4) + b4)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(valid_prediction_l4, w_final) + b_final)\n",
    "\n",
    "  test_prediction_l1 = tf.nn.relu(tf.matmul(tf_test_dataset, w1) + b1)\n",
    "  test_prediction_l2 = tf.nn.relu(tf.matmul(test_prediction_l1, w2) + b2)\n",
    "  test_prediction_l3 = tf.nn.relu(tf.matmul(test_prediction_l2, w3) + b3)\n",
    "  test_prediction_l4 = tf.nn.relu(tf.matmul(test_prediction_l3, w4) + b4)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(test_prediction_l4, w_final) + b_final)\n",
    "    \n",
    "  train_prediction_l1 = tf.nn.relu(tf.matmul(tf_train_dataset, w1) + b1)\n",
    "  train_prediction_l2 = tf.nn.relu(tf.matmul(train_prediction_l1, w2) + b2)\n",
    "  train_prediction_l3 = tf.nn.relu(tf.matmul(train_prediction_l2, w3) + b3)\n",
    "  train_prediction_l4 = tf.nn.relu(tf.matmul(train_prediction_l3, w4) + b4)\n",
    "  train_prediction = tf.nn.softmax(tf.matmul(train_prediction_l4, w_final) + b_final)\n",
    "\n",
    "original_train_labels = train_labels\n",
    "record_data_period = 500\n",
    "next_plot_index = 0\n",
    "validation_plot_data = []\n",
    "training_plot_data   = []\n",
    "minibatch_plot_data  = []\n",
    "\n",
    "fig, plot = plt.subplots(1,1)\n",
    "plot.set_xlabel('Steps')\n",
    "plot.set_ylabel('Accuracy')\n",
    "plot.set_xlim(0, num_epochs * num_steps * 1.5)\n",
    "plot.set_ylim(80,100)\n",
    "plot.grid(color='gray', linestyle='--', linewidth=1)\n",
    "fig.canvas.draw()\n",
    "# plot.plot(np.arange(next_plot_index) * record_data_period, np.arange(next_plot_index) * 100)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "      train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "      # print(\"==========\", \"Epoch \", epoch, \"==========\")\n",
    "  \n",
    "      for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_batch_dataset : batch_data, tf_batch_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, batch_prediction], feed_dict=feed_dict)\n",
    "        if (step % record_data_period == 0):\n",
    "          if math.isnan(l):\n",
    "            print(\"Loss is NAN, search diverged\")\n",
    "          \n",
    "          # Update the plot\n",
    "          validation_plot_data.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "          training_plot_data.append(accuracy(train_prediction.eval(), original_train_labels))\n",
    "          minibatch_plot_data.append(accuracy(predictions, batch_labels))\n",
    "          next_plot_index += 1\n",
    "          plot.plot(np.arange(next_plot_index) * record_data_period, training_plot_data, 'red')\n",
    "          # plot.plot(np.arange(next_plot_index) * record_data_period, minibatch_plot_data, 'orange')\n",
    "          plot.plot(np.arange(next_plot_index) * record_data_period, validation_plot_data, 'blue')\n",
    "          plot.set_xlabel(\"Steps\")\n",
    "          fig.canvas.draw()\n",
    "          \n",
    "          \n",
    "  \n",
    "      if (epoch == num_epochs - 1) :\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE\n",
    "----\n",
    "\n",
    "I attempted to match the \"best\" deep impl desribed in the note at the beginning of part 4 [repeated here](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595), but then I realized that they forgot to randomize their dataset just like I did in the begining :p so they really get ~93 (valid dataset). After realizing this I am okay with 91.1% and am calling it a day to move on with the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
