{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "\n",
    "# I want to re-randomize the inputs, I am not convinced they are shuffled well\n",
    "total_dataset = np.concatenate((train_dataset, valid_dataset, test_dataset), axis=0)\n",
    "total_labels = np.concatenate((train_labels, valid_labels, test_labels), axis=0)\n",
    "total_dataset, total_labels = randomize(total_dataset, total_labels)\n",
    "train_start = 0\n",
    "valid_start = train_dataset.shape[0]\n",
    "test_start = valid_start + valid_dataset.shape[0]\n",
    "\n",
    "train_dataset = total_dataset[train_start:valid_start, :]\n",
    "train_labels = total_labels[train_start:valid_start] \n",
    "\n",
    "valid_dataset = total_dataset[valid_start:test_start, :]\n",
    "valid_labels = total_labels[valid_start:test_start] \n",
    "\n",
    "test_dataset = total_dataset[test_start:, :]\n",
    "test_labels = total_labels[test_start:] \n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Base implementation for reference\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Epoch  0 ==========\n",
      "Minibatch loss at step 6249: 16.693886\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.3%\n",
      "========== Epoch  1 ==========\n",
      "Minibatch loss at step 6249: 1.918944\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 83.8%\n",
      "========== Epoch  2 ==========\n",
      "Minibatch loss at step 6249: 2.713311\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 81.1%\n",
      "Test accuracy: 81.1%\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_width = 1024\n",
    "batch_size = 32\n",
    "num_epochs = 3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random values following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  w1 = tf.Variable(\n",
    "      tf.truncated_normal([image_size * image_size, hidden_layer_width]))\n",
    "  b1 = tf.Variable(tf.zeros([hidden_layer_width]))\n",
    "    \n",
    "  w_final = tf.Variable(\n",
    "      tf.truncated_normal([hidden_layer_width, num_labels]))\n",
    "  b_final = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  layer1_activations = tf.nn.relu(tf.matmul(tf_train_dataset, w1) + b1)\n",
    "  final_activations = tf.matmul(layer1_activations, w_final) + b_final\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=final_activations))\n",
    "  \n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  train_prediction = tf.nn.softmax(final_activations)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "      tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, w1) + b1), w_final) + b_final)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "      tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, w1) + b1), w_final) + b_final)\n",
    "\n",
    "num_steps = train_dataset.shape[0] // batch_size\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      tf.global_variables_initializer().run()\n",
    "      print(\"==========\", \"Epoch \", epoch, \"==========\")\n",
    "  \n",
    "      for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step == num_steps - 1):\n",
    "          print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "          print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "          print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "  \n",
    "      if (epoch == num_epochs - 1) :\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1 (L2 Regularization)\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Epoch  0 ==========\n",
      "Minibatch loss at step 6249: 3.957988\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 84.8%\n",
      "========== Epoch  1 ==========\n",
      "Minibatch loss at step 6249: 3.804486\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 85.1%\n",
      "========== Epoch  2 ==========\n",
      "Minibatch loss at step 6249: 3.914960\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 86.2%\n",
      "Test accuracy: 85.7%\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_width = 1024\n",
    "batch_size = 32\n",
    "num_epochs = 3\n",
    "beta = 0.005 # l2 regularization param\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random values following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  w1 = tf.Variable(\n",
    "      tf.truncated_normal([image_size * image_size, hidden_layer_width]))\n",
    "  b1 = tf.Variable(tf.zeros([hidden_layer_width]))\n",
    "    \n",
    "  w_final = tf.Variable(\n",
    "      tf.truncated_normal([hidden_layer_width, num_labels]))\n",
    "  b_final = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  layer1_activations = tf.nn.relu(tf.matmul(tf_train_dataset, w1) + b1)\n",
    "  final_activations = tf.matmul(layer1_activations, w_final) + b_final\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=final_activations)) + beta * tf.nn.l2_loss(w1)\n",
    "  \n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  train_prediction = tf.nn.softmax(final_activations)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "      tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, w1) + b1), w_final) + b_final)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "      tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, w1) + b1), w_final) + b_final)\n",
    "\n",
    "num_steps = train_dataset.shape[0] // batch_size\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      tf.global_variables_initializer().run()\n",
    "      print(\"==========\", \"Epoch \", epoch, \"==========\")\n",
    "  \n",
    "      for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step == num_steps - 1):\n",
    "          print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "          print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "          print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "  \n",
    "      if (epoch == num_epochs - 1) :\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2 (Overfitting)\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "Answer: Accuracy drops from ~85% --> ~65%\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Epoch  0 ==========\n",
      "Minibatch loss at step 6249: 2.995165\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.1%\n",
      "========== Epoch  1 ==========\n",
      "Minibatch loss at step 6249: 2.993871\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.0%\n",
      "========== Epoch  2 ==========\n",
      "Minibatch loss at step 6249: 2.991153\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.3%\n",
      "Test accuracy: 68.8%\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_width = 1024\n",
    "batch_size = 32\n",
    "num_epochs = 3\n",
    "beta = 0.005 # l2 regularization param\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random values following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  w1 = tf.Variable(\n",
    "      tf.truncated_normal([image_size * image_size, hidden_layer_width]))\n",
    "  b1 = tf.Variable(tf.zeros([hidden_layer_width]))\n",
    "    \n",
    "  w_final = tf.Variable(\n",
    "      tf.truncated_normal([hidden_layer_width, num_labels]))\n",
    "  b_final = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  layer1_activations = tf.nn.relu(tf.matmul(tf_train_dataset, w1) + b1)\n",
    "  final_activations = tf.matmul(layer1_activations, w_final) + b_final\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=final_activations)) + beta * tf.nn.l2_loss(w1)\n",
    "  \n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  train_prediction = tf.nn.softmax(final_activations)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "      tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, w1) + b1), w_final) + b_final)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "      tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, w1) + b1), w_final) + b_final)\n",
    "\n",
    "num_steps = train_dataset.shape[0] // batch_size\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      tf.global_variables_initializer().run()\n",
    "      print(\"==========\", \"Epoch \", epoch, \"==========\")\n",
    "  \n",
    "      for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        # offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Only use 10 batches\n",
    "        offset = (step * batch_size) % (5 * batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step == num_steps - 1):\n",
    "          print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "          print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "          print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "  \n",
    "      if (epoch == num_epochs - 1) :\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3 (Dropout)\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "Answer: We go from ~65% accuracy to more around 74%\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfit + Dropout\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Epoch  0 ==========\n",
      "Minibatch loss at step 6249: 4.117795\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.5%\n",
      "========== Epoch  1 ==========\n",
      "Minibatch loss at step 6249: 4.291440\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.6%\n",
      "========== Epoch  2 ==========\n",
      "Minibatch loss at step 6249: 7.015709\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 73.5%\n",
      "Test accuracy: 72.9%\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_width = 1024\n",
    "batch_size = 32\n",
    "num_epochs = 3\n",
    "beta = 0.005 # l2 regularization param\n",
    "learn_rate = 0.1\n",
    "dropout_percent = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random values following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  w1 = tf.Variable(\n",
    "      tf.truncated_normal([image_size * image_size, hidden_layer_width]))\n",
    "  b1 = tf.Variable(tf.zeros([hidden_layer_width]))\n",
    "    \n",
    "  w_final = tf.Variable(\n",
    "      tf.truncated_normal([hidden_layer_width, num_labels]))\n",
    "  b_final = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  layer1_activations = tf.nn.dropout(tf.nn.relu(tf.matmul(tf_train_dataset, w1) + b1), 1.0 - dropout_percent)\n",
    "  final_activations = tf.matmul(layer1_activations, w_final) + b_final\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=final_activations)) + beta * tf.nn.l2_loss(w1)\n",
    "  \n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  train_prediction = tf.nn.softmax(final_activations)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "      tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, w1) + b1), w_final) + b_final)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "      tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, w1) + b1), w_final) + b_final)\n",
    "    \n",
    "num_steps = train_dataset.shape[0] // batch_size\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      tf.global_variables_initializer().run()\n",
    "      print(\"==========\", \"Epoch \", epoch, \"==========\")\n",
    "  \n",
    "      for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        # offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        offset = (step * batch_size) % (10 * batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step == num_steps - 1):\n",
    "          print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "          print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "          print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "  \n",
    "      if (epoch == num_epochs - 1) :\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good pipeline + dropout\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Epoch  0 ==========\n",
      "Minibatch loss at step 6249: 3.396955\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 85.9%\n",
      "========== Epoch  1 ==========\n",
      "Minibatch loss at step 6249: 0.695027\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 86.3%\n",
      "========== Epoch  2 ==========\n",
      "Minibatch loss at step 6249: 0.475265\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 86.7%\n",
      "Test accuracy: 86.4%\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_width = 1024\n",
    "batch_size = 32\n",
    "num_epochs = 3\n",
    "beta = 0.005 # l2 regularization param\n",
    "learn_rate = 0.1\n",
    "dropout_percent = 0.3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random values following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  w1 = tf.Variable(\n",
    "      tf.truncated_normal([image_size * image_size, hidden_layer_width]))\n",
    "  b1 = tf.Variable(tf.zeros([hidden_layer_width]))\n",
    "    \n",
    "  w_final = tf.Variable(\n",
    "      tf.truncated_normal([hidden_layer_width, num_labels]))\n",
    "  b_final = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  layer1_activations = tf.nn.dropout(tf.nn.relu(tf.matmul(tf_train_dataset, w1) + b1), 1.0 - dropout_percent)\n",
    "  final_activations = tf.matmul(layer1_activations, w_final) + b_final\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=final_activations)) + beta * tf.nn.l2_loss(w1)\n",
    "  \n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  train_prediction = tf.nn.softmax(final_activations)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "      tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, w1) + b1), w_final) + b_final)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "      tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, w1) + b1), w_final) + b_final)\n",
    "    \n",
    "num_steps = train_dataset.shape[0] // batch_size\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "      train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "\n",
    "      print(\"==========\", \"Epoch \", epoch, \"==========\")\n",
    "  \n",
    "      for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step == num_steps - 1):\n",
    "          print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "          print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "          print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "  \n",
    "      if (epoch == num_epochs - 1) :\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4 (Get Gud)\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    "Best Impl so far\n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Epoch  0 ==========\n",
      "\\\\\\\\\\\\\n",
      "Minibatch loss at step 1561: 1.298463\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.7%\n",
      "Training set accuracy: 90.0%\n",
      "////////////\n",
      "========== Epoch  1 ==========\n",
      "\\\\\\\\\\\\\n",
      "Minibatch loss at step 1561: 1.043907\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.3%\n",
      "Training set accuracy: 92.3%\n",
      "////////////\n",
      "========== Epoch  2 ==========\n",
      "\\\\\\\\\\\\\n",
      "Minibatch loss at step 1561: 0.939636\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.9%\n",
      "Training set accuracy: 93.5%\n",
      "////////////\n",
      "========== Epoch  3 ==========\n",
      "\\\\\\\\\\\\\n",
      "Minibatch loss at step 1561: 0.868906\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.1%\n",
      "Training set accuracy: 94.6%\n",
      "////////////\n",
      "========== Epoch  4 ==========\n",
      "\\\\\\\\\\\\\n",
      "Minibatch loss at step 1561: 0.744670\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.4%\n",
      "Training set accuracy: 95.5%\n",
      "////////////\n",
      "========== Epoch  5 ==========\n",
      "\\\\\\\\\\\\\n",
      "Minibatch loss at step 1561: 0.770170\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.6%\n",
      "Training set accuracy: 96.1%\n",
      "////////////\n",
      "========== Epoch  6 ==========\n",
      "\\\\\\\\\\\\\n",
      "Minibatch loss at step 1561: 0.703556\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.6%\n",
      "Training set accuracy: 96.4%\n",
      "////////////\n",
      "========== Epoch  7 ==========\n",
      "\\\\\\\\\\\\\n",
      "Minibatch loss at step 1561: 0.599541\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 91.6%\n",
      "Training set accuracy: 96.8%\n",
      "////////////\n",
      "========== Epoch  8 ==========\n",
      "\\\\\\\\\\\\\n",
      "Minibatch loss at step 1561: 0.637489\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.7%\n",
      "Training set accuracy: 97.0%\n",
      "////////////\n",
      "========== Epoch  9 ==========\n",
      "\\\\\\\\\\\\\n",
      "Minibatch loss at step 1561: 0.597370\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.8%\n",
      "Training set accuracy: 97.1%\n",
      "////////////\n",
      "Test accuracy: 91.4%\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_width = [2048, 1024]\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "beta = 0.0005 # l2 regularization param\n",
    "start_learn_rate = 0.2 # Seems to be about the max stable number\n",
    "end_learn_rate = 0.01\n",
    "dropout_percent = 0.0\n",
    "\n",
    "# Derived parameters\n",
    "num_steps = train_dataset.shape[0] // batch_size\n",
    "learn_rate_n_steps = num_steps * num_epochs\n",
    "learn_rate_decay = end_learn_rate / start_learn_rate\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_batch_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_batch_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  \n",
    "  tf_train_dataset = tf.constant(train_dataset)\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random values following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  \n",
    "  global_step = tf.Variable(0, trainable=False)  \n",
    "  w1 = tf.Variable(\n",
    "      tf.truncated_normal([image_size * image_size, hidden_layer_width[0]], stddev=np.sqrt(2.0 / hidden_layer_width[0])))\n",
    "  b1 = tf.Variable(tf.zeros([hidden_layer_width[0]]))\n",
    "\n",
    "  w2 = tf.Variable(\n",
    "      tf.truncated_normal([hidden_layer_width[0], hidden_layer_width[1]], stddev=np.sqrt(2.0 / hidden_layer_width[1])))\n",
    "  b2 = tf.Variable(tf.zeros([hidden_layer_width[1]]))\n",
    "    \n",
    "  w_final = tf.Variable(\n",
    "      tf.truncated_normal([hidden_layer_width[1], num_labels], stddev=np.sqrt(2.0 / num_labels)))\n",
    "  b_final = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  layer1_activations = tf.nn.dropout(tf.nn.relu(tf.matmul(tf_batch_dataset, w1) + b1), 1.0 - dropout_percent)\n",
    "  layer2_activations = tf.nn.dropout(tf.nn.relu(tf.matmul(layer1_activations, w2) + b2), 1.0 - dropout_percent)\n",
    "  final_activations = tf.matmul(layer2_activations, w_final) + b_final\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_batch_labels, logits=final_activations))\n",
    "  loss = loss + beta * (tf.nn.l2_loss(w1) + tf.nn.l2_loss(b1) + tf.nn.l2_loss(w2) + tf.nn.l2_loss(b2))\n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  learn_rate = tf.train.exponential_decay(start_learn_rate, global_step, learn_rate_n_steps, learn_rate_decay)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  batch_prediction = tf.nn.softmax(final_activations)\n",
    "  \n",
    "  valid_prediction_l1 = tf.nn.relu(tf.matmul(tf_valid_dataset, w1) + b1)\n",
    "  valid_prediction_l2 = tf.nn.relu(tf.matmul(valid_prediction_l1, w2) + b2)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(valid_prediction_l2, w_final) + b_final)\n",
    "\n",
    "  test_prediction_l1 = tf.nn.relu(tf.matmul(tf_test_dataset, w1) + b1)\n",
    "  test_prediction_l2 = tf.nn.relu(tf.matmul(test_prediction_l1, w2) + b2)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(test_prediction_l2, w_final) + b_final)\n",
    "    \n",
    "  train_prediction_l1 = tf.nn.relu(tf.matmul(tf_train_dataset, w1) + b1)\n",
    "  train_prediction_l2 = tf.nn.relu(tf.matmul(train_prediction_l1, w2) + b2)\n",
    "  train_prediction = tf.nn.softmax(tf.matmul(train_prediction_l2, w_final) + b_final)\n",
    "\n",
    "original_train_labels = train_labels\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "      train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "      print(\"==========\", \"Epoch \", epoch, \"==========\")\n",
    "  \n",
    "      for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_batch_dataset : batch_data, tf_batch_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, batch_prediction], feed_dict=feed_dict)\n",
    "        if (step == num_steps - 1):\n",
    "          print(\"\\\\\\\\\\\\\\\\\\\\\\\\\")\n",
    "          print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "          print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "          print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "          valid_prediction.eval(), valid_labels))\n",
    "          print(\"Training set accuracy: %.1f%%\" % accuracy(train_prediction.eval(), original_train_labels))\n",
    "          print(\"////////////\")\n",
    "  \n",
    "      if (epoch == num_epochs - 1) :\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support.' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option)\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width, fig.canvas.height);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to  previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        event.shiftKey = false;\n",
       "        // Send a \"J\" for go to next cell\n",
       "        event.which = 74;\n",
       "        event.keyCode = 74;\n",
       "        manager.command_mode();\n",
       "        manager.handle_keydown(event);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"638.888905813665\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 91.4%\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_width = [2048, 1024, 512, 512]\n",
    "batch_size = 128\n",
    "num_epochs = 30\n",
    "beta = 0.005  #0.005 # l2 regularization param\n",
    "start_learn_rate = 0.1 # Seems to be about the max stable number\n",
    "end_learn_rate = 0.001\n",
    "dropout_percent = [0.3, 0.2, 0.2, 0.1]\n",
    "\n",
    "# Derived parameters\n",
    "num_steps = train_dataset.shape[0] // batch_size\n",
    "learn_rate_n_steps = num_steps * num_epochs\n",
    "learn_rate_decay = end_learn_rate / start_learn_rate\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_batch_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_batch_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  \n",
    "  tf_train_dataset = tf.constant(train_dataset)\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random values following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  \n",
    "  global_step = tf.Variable(0, trainable=False)  \n",
    "  w1 = tf.Variable(\n",
    "      tf.truncated_normal([image_size * image_size, hidden_layer_width[0]], stddev=np.sqrt(2.0 / hidden_layer_width[0])))\n",
    "  b1 = tf.Variable(tf.zeros([hidden_layer_width[0]]))\n",
    "\n",
    "  w2 = tf.Variable(\n",
    "      tf.truncated_normal([hidden_layer_width[0], hidden_layer_width[1]], stddev=np.sqrt(2.0 / hidden_layer_width[1])))\n",
    "  b2 = tf.Variable(tf.zeros([hidden_layer_width[1]]))\n",
    "\n",
    "  w3 = tf.Variable(\n",
    "      tf.truncated_normal([hidden_layer_width[1], hidden_layer_width[2]], stddev=np.sqrt(2.0 / hidden_layer_width[2])))\n",
    "  b3 = tf.Variable(tf.zeros([hidden_layer_width[2]]))\n",
    "\n",
    "  w4 = tf.Variable(\n",
    "      tf.truncated_normal([hidden_layer_width[2], hidden_layer_width[3]], stddev=np.sqrt(2.0 / hidden_layer_width[3])))\n",
    "  b4 = tf.Variable(tf.zeros([hidden_layer_width[3]]))\n",
    "    \n",
    "  w_final = tf.Variable(\n",
    "      tf.truncated_normal([hidden_layer_width[3], num_labels], stddev=np.sqrt(2.0 / num_labels)))\n",
    "  b_final = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  layer1_activations = tf.nn.dropout(tf.nn.relu(tf.matmul(tf_batch_dataset, w1) + b1), 1.0 - dropout_percent[0])\n",
    "  layer2_activations = tf.nn.dropout(tf.nn.relu(tf.matmul(layer1_activations, w2) + b2), 1.0 - dropout_percent[1])\n",
    "  layer3_activations = tf.nn.dropout(tf.nn.relu(tf.matmul(layer2_activations, w3) + b3), 1.0 - dropout_percent[2])\n",
    "  layer4_activations = tf.nn.dropout(tf.nn.relu(tf.matmul(layer3_activations, w4) + b4), 1.0 - dropout_percent[3])\n",
    "  final_activations = tf.matmul(layer4_activations, w_final) + b_final\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_batch_labels, logits=final_activations))\n",
    "  loss = loss + beta * (tf.nn.l2_loss(w1) + tf.nn.l2_loss(b1) + \n",
    "                        tf.nn.l2_loss(w2) + tf.nn.l2_loss(b2) + \n",
    "                        tf.nn.l2_loss(w3) + tf.nn.l2_loss(b3) + \n",
    "                        tf.nn.l2_loss(w4) + tf.nn.l2_loss(b4))\n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  learn_rate = tf.train.exponential_decay(start_learn_rate, global_step, learn_rate_n_steps, learn_rate_decay)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  batch_prediction = tf.nn.softmax(final_activations)\n",
    "  \n",
    "  valid_prediction_l1 = tf.nn.relu(tf.matmul(tf_valid_dataset, w1) + b1)\n",
    "  valid_prediction_l2 = tf.nn.relu(tf.matmul(valid_prediction_l1, w2) + b2)\n",
    "  valid_prediction_l3 = tf.nn.relu(tf.matmul(valid_prediction_l2, w3) + b3)\n",
    "  valid_prediction_l4 = tf.nn.relu(tf.matmul(valid_prediction_l3, w4) + b4)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(valid_prediction_l4, w_final) + b_final)\n",
    "\n",
    "  test_prediction_l1 = tf.nn.relu(tf.matmul(tf_test_dataset, w1) + b1)\n",
    "  test_prediction_l2 = tf.nn.relu(tf.matmul(test_prediction_l1, w2) + b2)\n",
    "  test_prediction_l3 = tf.nn.relu(tf.matmul(test_prediction_l2, w3) + b3)\n",
    "  test_prediction_l4 = tf.nn.relu(tf.matmul(test_prediction_l3, w4) + b4)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(test_prediction_l4, w_final) + b_final)\n",
    "    \n",
    "  train_prediction_l1 = tf.nn.relu(tf.matmul(tf_train_dataset, w1) + b1)\n",
    "  train_prediction_l2 = tf.nn.relu(tf.matmul(train_prediction_l1, w2) + b2)\n",
    "  train_prediction_l3 = tf.nn.relu(tf.matmul(train_prediction_l2, w3) + b3)\n",
    "  train_prediction_l4 = tf.nn.relu(tf.matmul(train_prediction_l3, w4) + b4)\n",
    "  train_prediction = tf.nn.softmax(tf.matmul(train_prediction_l4, w_final) + b_final)\n",
    "\n",
    "original_train_labels = train_labels\n",
    "record_data_period = 500\n",
    "next_plot_index = 0\n",
    "validation_plot_data = []\n",
    "training_plot_data   = []\n",
    "minibatch_plot_data  = []\n",
    "\n",
    "fig, plot = plt.subplots(1,1)\n",
    "plot.set_xlabel('Steps')\n",
    "plot.set_ylabel('Accuracy')\n",
    "plot.set_xlim(0, num_epochs * num_steps * 1.5)\n",
    "plot.set_ylim(80,100)\n",
    "plot.grid(color='gray', linestyle='--', linewidth=1)\n",
    "fig.canvas.draw()\n",
    "# plot.plot(np.arange(next_plot_index) * record_data_period, np.arange(next_plot_index) * 100)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "      train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "      # print(\"==========\", \"Epoch \", epoch, \"==========\")\n",
    "  \n",
    "      for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_batch_dataset : batch_data, tf_batch_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, batch_prediction], feed_dict=feed_dict)\n",
    "        if (step % record_data_period == 0):\n",
    "          if math.isnan(l):\n",
    "            print(\"Loss is NAN, search diverged\")\n",
    "          \n",
    "          # Update the plot\n",
    "          validation_plot_data.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "          training_plot_data.append(accuracy(train_prediction.eval(), original_train_labels))\n",
    "          minibatch_plot_data.append(accuracy(predictions, batch_labels))\n",
    "          next_plot_index += 1\n",
    "          plot.plot(np.arange(next_plot_index) * record_data_period, training_plot_data, 'red')\n",
    "          # plot.plot(np.arange(next_plot_index) * record_data_period, minibatch_plot_data, 'orange')\n",
    "          plot.plot(np.arange(next_plot_index) * record_data_period, validation_plot_data, 'blue')\n",
    "          plot.set_xlabel(\"Steps\")\n",
    "          fig.canvas.draw()\n",
    "          \n",
    "          \n",
    "  \n",
    "      if (epoch == num_epochs - 1) :\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE\n",
    "----\n",
    "\n",
    "I attempted to match the \"best\" deep impl desribed in the note at the beginning of part 4 [repeated here](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595), but then I realized that they forgot to randomize their dataset just like I did in the begining :p so they really get ~93 (valid dataset). After realizing this I am okay with 91.1% and am calling it a day to move on with the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
